{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2342813f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Student Name: Akhila Mora\n",
    "## Student Email: akhila.mora@ou.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e6ab65",
   "metadata": {},
   "source": [
    "# Project 3: The Smart City Slicker\n",
    "\n",
    "Imagine you are a stakeholder in a rising Smart City and want to know more about themes and concepts about existing smart cities. You also want to know where does your smart city place among others. In this project, you will perform \n",
    "exploratory data analysis, often shortened to EDA, to examine a data from the [2015 Smart City Challenge](https://www.transportation.gov/smartcity) to find facts about the data and communicating those facts through text analysis and visualizations.\n",
    "\n",
    "In order to explore the data and visualize it, some modifications might need to be made to the data along the way. This is often referred to as data preprocessing or cleaning.\n",
    "Though data preprocessing is technically different from EDA, EDA often exposes problems with the data that need to be fixed in order to continue exploring.\n",
    "Because of this tight coupling, you have to clean the data as necessary to help understand the data.\n",
    "\n",
    "In this project, you will apply your knowledge about data cleaning, machine learning, visualizations, and databases to explore smart city applications.\n",
    "\n",
    "**Part 1** of the notebook will explore and clean the data. \\\n",
    "**Part 2** will take the results of the preprocessed data to create models and visualizations.\n",
    "\n",
    "Empty cells are code cells. \n",
    "Cells denoted with [Your Answer Here] are markdown cells.\n",
    "Edit and add as many cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8dcba1",
   "metadata": {},
   "source": [
    "Output file for this notebook is shown as a table for display purposes. Note: The city name can be Norman, OK or OK Norman.\n",
    "\n",
    "| city | raw text | clean text | clusterid | topicids | summary | keywords|\n",
    "| -- | -- | -- | -- | -- | -- | -- |\n",
    "|Norman, OK | Test, test , and testing. | test test test | 0 | T1, T2| test | test |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd47ce",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The Dataset: 2015 Smart City Challenge Applicants (non-finalist).\n",
    "In this project you will use the applicant's PDFs as a dataset.\n",
    "The dataset is from the U.S Department of Transportation Smart City Challenge.\n",
    "\n",
    "On the website page for the data, you can find some basic information about the challenge. This is an interesting dataset. Think of the questions that you might be able to answer! A few could be:\n",
    "\n",
    "1. Can I identify frequently occurring words that could be removed during data preprocessing?\n",
    "2. Where are the applicants from?\n",
    "3. Are there multiple entries for the same city in different applicantions?\n",
    "4. What are the major themes and concepts from the smart city applicants?\n",
    "\n",
    "Let's load the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aace966",
   "metadata": {},
   "source": [
    "## Loading and Handling files\n",
    "\n",
    "Load data from `smartcity/`. \n",
    "\n",
    "To extract the data from the pdf files, use the [pypdf.pdf.PdfFileReader](https://pypdf.readthedocs.io/en/stable/index.html) class.\n",
    "It will allow you to extract pages and pdf files and add them to a data structure (dataframe, list, dictionary, etc).\n",
    "To install the module, use the command `pipenv install pypdf`.\n",
    "You only need to handle PDF files, handling docx is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef0afb30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.1\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "import pandas as pd\n",
    "import os\n",
    "import pypdf\n",
    "print(pypdf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ed6e32",
   "metadata": {},
   "source": [
    "Create a data structure to add the city name and raw text. You can choose to split the city name from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e4905f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GA Brookhaven\n",
      "Brookhaven GA\n",
      "NY Buffalo\n",
      "Buffalo NY\n",
      "CA Riverside\n",
      "Riverside CA\n",
      "AZ Scottsdale AZ\n",
      "Scottsdale AZ AZ\n",
      "FL Jacksonville\n",
      "Jacksonville FL\n",
      "LA New Orleans\n",
      "New Orleans LA\n",
      "AL Montgomery\n",
      "Montgomery AL\n",
      "MI Port Huron and Marysville\n",
      "Port Huron and Marysville MI\n",
      "WA Seattle\n",
      "Seattle WA\n",
      "LA Shreveport\n",
      "Shreveport LA\n",
      "WA Spokane\n",
      "Spokane WA\n",
      "IN Indianapolis\n",
      "Indianapolis IN\n",
      "AL Birmingham\n",
      "Birmingham AL\n",
      "LA Baton Rouge\n",
      "Baton Rouge LA\n",
      "FL Miami\n",
      "Miami FL\n",
      "CA Oceanside\n",
      "Oceanside CA\n",
      "CA San Jose_0\n",
      "San Jose_0 CA\n",
      "NE Lincoln\n",
      "Lincoln NE\n",
      "MA Boston\n",
      "Boston MA\n",
      "CA Sacramento\n",
      "Sacramento CA\n",
      "VA Richmond\n",
      "Richmond VA\n",
      "GA Atlanta\n",
      "Atlanta GA\n",
      "NY Rochester\n",
      "Rochester NY\n",
      "TN Memphis\n",
      "Memphis TN\n",
      "NC Raleigh\n",
      "Raleigh NC\n",
      "NY Albany Troy Schenectady Saratoga Springs\n",
      "Albany Troy Schenectady Saratoga Springs NY\n",
      "OH Cleveland\n",
      "Cleveland OH\n",
      "NC Charlotte\n",
      "Charlotte NC\n",
      "NJ Jersey City\n",
      "Jersey City NJ\n",
      "CA Chula Vista\n",
      "Chula Vista CA\n",
      "CA Long Beach\n",
      "Long Beach CA\n",
      "MI Detroit\n",
      "Detroit MI\n",
      "IA Des Moines\n",
      "Des Moines IA\n",
      "MO St. Louis\n",
      "St. Louis MO\n",
      "NE Omaha\n",
      "Omaha NE\n",
      "OH Akron\n",
      "Akron OH\n",
      "VA Newport News\n",
      "Newport News VA\n",
      "NY Mt Vernon Yonkers New Rochelle\n",
      "Mt Vernon Yonkers New Rochelle NY\n",
      "CA Fremont\n",
      "Fremont CA\n",
      "MD Baltimore\n",
      "Baltimore MD\n",
      "SC Greenville\n",
      "Greenville SC\n",
      "CT NewHaven\n",
      "NewHaven CT\n",
      "TX Lubbock\n",
      "Lubbock TX\n",
      "CA Fresno\n",
      "Fresno CA\n",
      "CA Oakland\n",
      "Oakland CA\n",
      "TN Chattanooga\n",
      "Chattanooga TN\n",
      "RI Providence\n",
      "Providence RI\n",
      "AK Anchorage\n",
      "Anchorage AK\n",
      "AZ Tucson\n",
      "Tucson AZ\n",
      "MN Minneapolis St Paul\n",
      "Minneapolis St Paul MN\n",
      "NV Reno\n",
      "Reno NV\n",
      "OH Toledo\n",
      "Toledo OH\n",
      "NC Greensboro\n",
      "Greensboro NC\n",
      "OH Canton\n",
      "Canton OH\n",
      "NV Las Vegas\n",
      "Las Vegas NV\n",
      "TN Nashville\n",
      "Nashville TN\n",
      "OK Oklahoma City\n",
      "Oklahoma City OK\n",
      "WI Madison\n",
      "Madison WI\n",
      "NJ Newark\n",
      "Newark NJ\n",
      "KY Louisville\n",
      "Louisville KY\n",
      "FL St. Petersburg\n",
      "St. Petersburg FL\n",
      "CA Moreno Valley\n",
      "Moreno Valley CA\n",
      "FL Tampa\n",
      "Tampa FL\n",
      "VA Norfolk\n",
      "Norfolk VA\n",
      "DC_0\n",
      " DC_0\n",
      "FL Orlando\n",
      "Orlando FL\n",
      "VA Virginia Beach\n",
      "Virginia Beach VA\n",
      "OK Tulsa\n",
      "Tulsa OK\n",
      "                                             raw_text               city\n",
      "0   “Buford Highway through DeKalb County is the m...      Brookhaven GA\n",
      "1    \\n  \\nU.S. Department of Transportation \\nNot...         Buffalo NY\n",
      "2   CITY OF RIVERSIDE\\nCALIFORNIA\\nApplication For...       Riverside CA\n",
      "3     \\n  \\n \\n \\n \\nFederal Agency Name:   U.S. D...   Scottsdale AZ AZ\n",
      "4   Beyond Traffic: The Smart City Challenge \\nCon...    Jacksonville FL\n",
      "..                                                ...                ...\n",
      "63  City of Norfolk, VA\\n*\\nResponse Proposal to U...         Norfolk VA\n",
      "64   \\n   \\n    \\n \\nSmart  DC \\nMaking  the Distr...               DC_0\n",
      "65  BEYOND TRAFFIC: THE SMART CITY CHALLENGE - VIS...         Orlando FL\n",
      "66    \\n1.  Project Vision  .........................  Virginia Beach VA\n",
      "67  1 of 27 \\n Executive summary : Creating a Vibr...           Tulsa OK\n",
      "\n",
      "[68 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "dataframes = []\n",
    "c_names = []\n",
    "for f in os.listdir('./smartcity'):\n",
    "    #data = []\n",
    "    name = os.path.splitext(f)[0]\n",
    "    fp = os.path.join('./smartcity',f)\n",
    "    #print(fp)\n",
    "\n",
    "    if name != \".ipynb_checkpoints\" and os.path.splitext(f)[1] == \".pdf\":\n",
    "        t_nm = f.split('.pdf')[0]\n",
    "        print(t_nm)\n",
    "        req_nm = ' '.join(t_nm.split(\" \")[1:]) +\" \"+ t_nm.split(\" \")[0]\n",
    "        print(req_nm)\n",
    "        c_names.append(req_nm)\n",
    "        with open(fp, 'rb') as file:\n",
    "            pdf_reader = PdfReader(file)\n",
    "            text = ''\n",
    "            #print(len(pdf_reader.pages))\n",
    "            for p in range(len(pdf_reader.pages)):\n",
    "                pg = pdf_reader.pages[p]\n",
    "                pg_text=pg.extract_text()\n",
    "                if 'Contents' not in pg_text: \n",
    "                    text = text + pg.extract_text()\n",
    "                    #print(text)\n",
    "                    #data.append({'text':text})\n",
    "            dataframes.append(text)\n",
    "\n",
    "            #df = pd.DataFrame(data)\n",
    "            #dataframes.append(df)\n",
    "\n",
    "data = {'raw_text':dataframes}\n",
    "df = pd.DataFrame(data)\n",
    "df['city'] = c_names\n",
    "print(df)\n",
    "#df_combined = pd.concat(dataframes, ignore_index = True)\n",
    "#print(df_combined)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5019a8c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cleaning Up PDFs\n",
    "\n",
    "One of the more frustrating aspects of PDF is loading the data into a readable format. The first order of business will be to preprocess the data. To start, you can use code provided by Text Analytics with Python, [Chapter 3](https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch03%20-%20Processing%20and%20Understanding%20Text/Ch03a%20-%20Text%20Wrangling.ipynb): [contractions.py](https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch05%20-%20Text%20Classification/contractions.py) (Pages 136-137), and [text_normalizer.py](https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch05%20-%20Text%20Classification/text_normalizer.py) (Pages 155-156). Feel free to download the scripts or add the code directly to the notebook (please note this code is performed on dataframes).\n",
    "\n",
    "In addition to the data cleaning provided by the textbook, you will need to:\n",
    "1. Consider removing terms that may effect clustering and topic modeling. Words to consider are cities, states, common words (smart, city, page, etc.). Keep in mind n-gram combinations are important; this can also be revisited later depending on your model's performance.\n",
    "2. Check the data to remove applicants that text was not processed correctly. Do not remove more than 15 cities from the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8142e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import unicodedata\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "import collections\n",
    "import en_core_web_md\n",
    "#from textblob import Word\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "# nlp_vec = spacy.load('en_vectors_web_lg', parse=True, tag=True, entity=True)\n",
    "\n",
    "\n",
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    if bool(soup.find()):\n",
    "        [s.extract() for s in soup(['iframe', 'script'])]\n",
    "        stripped_text = soup.get_text()\n",
    "        stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    else:\n",
    "        stripped_text = text\n",
    "    return stripped_text\n",
    "\n",
    "\n",
    "#def correct_spellings_textblob(tokens):\n",
    "#\treturn [Word(token).correct() for token in tokens]  \n",
    "\n",
    "\n",
    "def simple_porter_stemming(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_repeated_characters(tokens):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "            \n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens\n",
    "\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKC', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]|\\[|\\]' if not remove_digits else r'[^a-zA-Z\\s]|\\[|\\]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False, stopwords=stopword_list):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_stemming=False, text_lemmatization=True, \n",
    "                     special_char_removal=True, remove_digits=True,\n",
    "                     stopword_removal=True, stopwords=stopword_list):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        \n",
    "        \n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "\n",
    "        # remove extra newlines\n",
    "        doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "\n",
    "        # stem text\n",
    "        if text_stemming and not text_lemmatization:\n",
    "        \tdoc = simple_porter_stemming(doc)\n",
    "\n",
    "        # remove special characters and\\or digits    \n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "\n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "\n",
    "         # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case, stopwords=stopwords)\n",
    "\n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        doc = doc.strip()\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "         \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d550b357-c834-4a9d-95f2-24d4dcd5f3d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/akhila/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('city', 9104), ('smart', 5588), ('data', 4888), ('transportation', 3964), ('transit', 2624), ('•', 2423), ('traffic', 2249), ('system', 2223), ('public', 2176), ('.', 2038)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "corpus = df['raw_text'][0:68]\n",
    "\n",
    "# Tokenize and remove stopwords\n",
    "tokens = [word.lower() for sent in corpus for word in sent.split() if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "# Generate frequency distribution\n",
    "freq_dist = nltk.FreqDist(tokens)\n",
    "\n",
    "# Print most common words\n",
    "print(freq_dist.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c984761-e34e-40e4-9cfe-8e0d0eebb5aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     “Buford Highway through DeKalb County is the m...\n",
      "1      \\n  \\nU.S. Department of Transportation \\nNot...\n",
      "2     CITY OF RIVERSIDE\\nCALIFORNIA\\nApplication For...\n",
      "3       \\n  \\n \\n \\n \\nFederal Agency Name:   U.S. D...\n",
      "4     Beyond Traffic: The Smart City Challenge \\nCon...\n",
      "                            ...                        \n",
      "63    City of Norfolk, VA\\n*\\nResponse Proposal to U...\n",
      "64     \\n   \\n    \\n \\nSmart  DC \\nMaking  the Distr...\n",
      "65    BEYOND TRAFFIC: THE SMART CITY CHALLENGE - VIS...\n",
      "66      \\n1.  Project Vision  .........................\n",
      "67    1 of 27 \\n Executive summary : Creating a Vibr...\n",
      "Name: raw_text, Length: 68, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['raw_text'][0:68])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bbfac77-7b17-43a7-b940-f75a074315e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "req_dforf1 = normalize_corpus(df['raw_text'][0:68], html_stripping=False, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_stemming=False, text_lemmatization=True, \n",
    "                     special_char_removal=True, remove_digits=True,\n",
    "                     stopword_removal=True, stopwords=stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e77d2f5-18db-4565-99d1-9b961ff49b49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    }
   ],
   "source": [
    "print(len(req_dforf1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1473a3e3",
   "metadata": {},
   "source": [
    "#### Add the cleaned text to the structure you created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3737fe02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(req_dforf1))\n",
    "df['clean_text'] = req_dforf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d60497d-2ce7-4bc7-822e-d1ecc83c18f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "860f591e-09fc-4186-ad2e-1bb3a34e91c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages (from scikit-learn) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87b9f86e-3ea7-4292-828d-c40e1bcc2474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector = TfidfVectorizer(ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2981b876-84d5-49a4-8737-fdef02049bf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c3eef60-dd26-4e93-a331-43016c1b4405",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_vector = vector.fit_transform(df['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8f30581-6129-462a-8113-b19e8f1dd693",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<68x714926 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 963865 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b437063-f7e5-425e-9922-572a3ab3a062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8078c182-6689-4474-a2b1-1c7004a9870d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kmean = KMeans(n_clusters=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb6d0f2c-aef2-4abf-ab05-98a59ec2c83c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=9)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=9)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(n_clusters=9)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmean.fit(train_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4e6a7ca-da46-4bab-b555-b3931edfcaa4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 2, 2, 2, 5, 5, 5, 5, 5, 5, 2, 1, 7, 5, 2, 5, 5, 0, 5, 2, 5, 4,\n",
       "       7, 2, 5, 2, 2, 2, 8, 7, 5, 5, 1, 2, 2, 1, 5, 3, 2, 2, 2, 2, 0, 2,\n",
       "       5, 2, 5, 2, 2, 2, 0, 0, 2, 5, 5, 1, 6, 5, 8, 2, 2, 0, 2, 5, 2, 2,\n",
       "       5, 6], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmean.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be868360-0f38-4f3d-a14e-fb4d97967632",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "721e43cb-fe16-4093-808e-7b0a5f2a571c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = kmean.predict(train_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81cbddeb-9288-4950-8f96-460e30794da9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.12378281736070233"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette_avg9 = silhouette_score(train_vector, labels)\n",
    "silhouette_avg9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e5567ae-5044-4e3a-b1eb-1278c9547744",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7f299c3-c9f5-4568-b85e-87f95a0912b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Calinski and Harabasz score is 9: 1.170837713307526\n",
      "The Davies-Bouldin score is 9:  3.2808945615202623\n"
     ]
    }
   ],
   "source": [
    "ch_score9 = calinski_harabasz_score(train_vector.toarray(), labels)\n",
    "\n",
    "# Compute the Davies-Bouldin score for the clustering result\n",
    "db_score9 = davies_bouldin_score(train_vector.toarray(), labels)\n",
    "\n",
    "print(\"The Calinski and Harabasz score is 9:\", ch_score9)\n",
    "print(\"The Davies-Bouldin score is 9: \", db_score9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4babc470-26b4-4328-9a8e-83550a09d66c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score 18: 0.00048224800770234647\n",
      "The Calinski and Harabasz score is 18: 1.0931813213798938\n",
      "The Davies-Bouldin score is 18:  0.9273446098570951\n"
     ]
    }
   ],
   "source": [
    "kmean = KMeans(n_clusters=18)\n",
    "kmean.fit(train_vector)\n",
    "labels = kmean.predict(train_vector)\n",
    "silhouette_avg18 = silhouette_score(train_vector, labels)\n",
    "print(\"Silhouette score 18:\",silhouette_avg18)\n",
    "ch_score18 = calinski_harabasz_score(train_vector.toarray(), labels)\n",
    "\n",
    "db_score18 = davies_bouldin_score(train_vector.toarray(), labels)\n",
    "\n",
    "print(\"The Calinski and Harabasz score is 18:\", ch_score18)\n",
    "print(\"The Davies-Bouldin score is 18: \", db_score18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecaaa1fd-5b0f-40de-ae39-ea1c10acff9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score 18: -0.0008394504056384801\n",
      "The Calinski and Harabasz score is 18: 1.1293725526318013\n",
      "The Davies-Bouldin score is 18:  0.895044187756752\n"
     ]
    }
   ],
   "source": [
    "kmean = KMeans(n_clusters=36)\n",
    "kmean.fit(train_vector)\n",
    "labels = kmean.predict(train_vector)\n",
    "silhouette_avg36 = silhouette_score(train_vector, labels)\n",
    "print(\"Silhouette score 18:\",silhouette_avg36)\n",
    "ch_score36 = calinski_harabasz_score(train_vector.toarray(), labels)\n",
    "\n",
    "db_score36 = davies_bouldin_score(train_vector.toarray(), labels)\n",
    "\n",
    "print(\"The Calinski and Harabasz score is 18:\", ch_score36)\n",
    "print(\"The Davies-Bouldin score is 18: \", db_score36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c602c83d-9147-47b7-9240-3e9c76e20bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score 9: 0.014942223766580564\n",
      "The Calinski and Harabasz score is 9: 1.2683450500290385\n",
      "The Davies-Bouldin score is 9:  2.753847577121421\n"
     ]
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "Z = linkage(train_vector.toarray(), method='ward')\n",
    "labels = fcluster(Z, t=9, criterion='maxclust')\n",
    "                  \n",
    "silhouette_avg9 = silhouette_score(train_vector, labels)\n",
    "print(\"Silhouette score 9:\",silhouette_avg9)\n",
    "ch_score9 = calinski_harabasz_score(train_vector.toarray(), labels)\n",
    "\n",
    "db_score9 = davies_bouldin_score(train_vector.toarray(), labels)\n",
    "\n",
    "print(\"The Calinski and Harabasz score is 9:\", ch_score9)\n",
    "print(\"The Davies-Bouldin score is 9: \", db_score9)                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2445d775-0288-40f7-bf82-bc21eafe128e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score 18: 0.008730683304876937\n",
      "The Calinski and Harabasz score is 18: 1.2239722006234135\n",
      "The Davies-Bouldin score is 18:  2.1036548709699487\n"
     ]
    }
   ],
   "source": [
    "Z = linkage(train_vector.toarray(), method='ward')\n",
    "labels = fcluster(Z, t=18, criterion='maxclust')\n",
    "                  \n",
    "silhouette_avg18 = silhouette_score(train_vector, labels)\n",
    "print(\"Silhouette score 18:\",silhouette_avg18)\n",
    "ch_score18 = calinski_harabasz_score(train_vector.toarray(), labels)\n",
    "\n",
    "db_score18 = davies_bouldin_score(train_vector.toarray(), labels)\n",
    "\n",
    "print(\"The Calinski and Harabasz score is 18:\", ch_score18)\n",
    "print(\"The Davies-Bouldin score is 18: \", db_score18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8f1de59-a495-4ac3-b4f9-7059044d3bf2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score 36: -0.0021525454066276253\n",
      "The Calinski and Harabasz score is 36: 1.2439593034433338\n",
      "The Davies-Bouldin score is 36:  1.348871215464741\n"
     ]
    }
   ],
   "source": [
    "Z = linkage(train_vector.toarray(), method='ward')\n",
    "labels = fcluster(Z, t=36, criterion='maxclust')\n",
    "                  \n",
    "silhouette_avg36 = silhouette_score(train_vector, labels)\n",
    "print(\"Silhouette score 36:\",silhouette_avg36)\n",
    "ch_score36 = calinski_harabasz_score(train_vector.toarray(), labels)\n",
    "\n",
    "db_score36 = davies_bouldin_score(train_vector.toarray(), labels)\n",
    "\n",
    "print(\"The Calinski and Harabasz score is 36:\", ch_score36)\n",
    "print(\"The Davies-Bouldin score is 36: \", db_score36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bad9bfc4-5a7d-4082-97e1-759eb3f55edc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Create a DBSCAN object\n",
    "dbscan = DBSCAN(eps=1.5, min_samples=5)\n",
    "\n",
    "# Fit the DBSCAN object on the vectorized data\n",
    "dbscan.fit(train_vector)\n",
    "\n",
    "# Predict the cluster labels for the vectorized data\n",
    "labels = dbscan.labels_\n",
    "\n",
    "# Count the number of clusters\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c8bb2ac-9834-418b-94f1-e5be82c95bf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "# Generate random data\n",
    "X = train_vector\n",
    "\n",
    "# Define range of k values to evaluate\n",
    "k_values = range(2, 24)\n",
    "\n",
    "# Initialize lists to store results\n",
    "ch_scores_kmeans = []\n",
    "silhouette_scores_kmeans = []\n",
    "davies_bouldin_scores_kmeans = []\n",
    "\n",
    "ch_scores_linkage = []\n",
    "silhouette_scores_linkage = []\n",
    "davies_bouldin_scores_linkage = []\n",
    "\n",
    "# Iterate over k values\n",
    "for k in k_values:\n",
    "    # Fit k-means model\n",
    "    kmeans_model = KMeans(n_clusters=k, random_state=42).fit(X)\n",
    "    \n",
    "    # Calculate metrics for k-means\n",
    "    ch_score_kmeans = calinski_harabasz_score(X.toarray(), kmeans_model.labels_)\n",
    "    silhouette_score_kmeans = silhouette_score(X, kmeans_model.labels_)\n",
    "    davies_bouldin_score_kmeans = davies_bouldin_score(X.toarray(), kmeans_model.labels_)\n",
    "    \n",
    "    # Append results to lists\n",
    "    ch_scores_kmeans.append(ch_score_kmeans)\n",
    "    silhouette_scores_kmeans.append(silhouette_score_kmeans)\n",
    "    davies_bouldin_scores_kmeans.append(davies_bouldin_score_kmeans)\n",
    "    \n",
    "    # Perform hierarchical clustering with linkage\n",
    "    linkage_matrix = linkage(X.toarray(), method='ward')\n",
    "    linkage_labels = fcluster(linkage_matrix, k, criterion='maxclust')\n",
    "    \n",
    "    # Calculate metrics for linkage\n",
    "    ch_score_linkage = calinski_harabasz_score(X.toarray(), linkage_labels)\n",
    "    silhouette_score_linkage = silhouette_score(X, linkage_labels)\n",
    "    davies_bouldin_score_linkage = davies_bouldin_score(X.toarray(), linkage_labels)\n",
    "    \n",
    "    # Append results to lists\n",
    "    ch_scores_linkage.append(ch_score_linkage)\n",
    "    silhouette_scores_linkage.append(silhouette_score_linkage)\n",
    "    davies_bouldin_scores_linkage.append(davies_bouldin_score_linkage)\n",
    "    \n",
    "# Create DataFrame with results\n",
    "results_df = pd.DataFrame({\n",
    "    'K': k_values,\n",
    "    'CH Score (K-Means)': ch_scores_kmeans,\n",
    "    'Silhouette Score (K-Means)': silhouette_scores_kmeans,\n",
    "    'Davies-Bouldin Score (K-Means)': davies_bouldin_scores_kmeans,\n",
    "    'CH Score (Linkage)': ch_scores_linkage,\n",
    "    'Silhouette Score (Linkage)': silhouette_scores_linkage,\n",
    "    'Davies-Bouldin Score (Linkage)': davies_bouldin_scores_linkage\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e9cce36-0d66-4931-8bd6-b4252b04adf2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>K</th>\n",
       "      <th>CH Score (K-Means)</th>\n",
       "      <th>Silhouette Score (K-Means)</th>\n",
       "      <th>Davies-Bouldin Score (K-Means)</th>\n",
       "      <th>CH Score (Linkage)</th>\n",
       "      <th>Silhouette Score (Linkage)</th>\n",
       "      <th>Davies-Bouldin Score (Linkage)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1.203952</td>\n",
       "      <td>0.046406</td>\n",
       "      <td>2.220843</td>\n",
       "      <td>1.337333</td>\n",
       "      <td>0.036296</td>\n",
       "      <td>3.559832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1.278319</td>\n",
       "      <td>-0.004435</td>\n",
       "      <td>4.913963</td>\n",
       "      <td>1.324281</td>\n",
       "      <td>0.031307</td>\n",
       "      <td>2.464872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1.199890</td>\n",
       "      <td>0.010224</td>\n",
       "      <td>4.661496</td>\n",
       "      <td>1.320808</td>\n",
       "      <td>0.031804</td>\n",
       "      <td>1.720088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1.153897</td>\n",
       "      <td>-0.102977</td>\n",
       "      <td>4.439481</td>\n",
       "      <td>1.312459</td>\n",
       "      <td>0.012864</td>\n",
       "      <td>3.438031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>1.166683</td>\n",
       "      <td>-0.054472</td>\n",
       "      <td>2.715054</td>\n",
       "      <td>1.301557</td>\n",
       "      <td>0.012811</td>\n",
       "      <td>3.306187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>1.150705</td>\n",
       "      <td>-0.101835</td>\n",
       "      <td>3.686196</td>\n",
       "      <td>1.291308</td>\n",
       "      <td>0.014895</td>\n",
       "      <td>3.055870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>1.156645</td>\n",
       "      <td>-0.002306</td>\n",
       "      <td>2.934716</td>\n",
       "      <td>1.278249</td>\n",
       "      <td>0.014904</td>\n",
       "      <td>2.996231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>1.185117</td>\n",
       "      <td>0.013614</td>\n",
       "      <td>2.404045</td>\n",
       "      <td>1.268345</td>\n",
       "      <td>0.014942</td>\n",
       "      <td>2.753848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>1.140845</td>\n",
       "      <td>-0.095776</td>\n",
       "      <td>1.933558</td>\n",
       "      <td>1.260104</td>\n",
       "      <td>0.014885</td>\n",
       "      <td>2.561505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>1.110942</td>\n",
       "      <td>-0.115641</td>\n",
       "      <td>1.742494</td>\n",
       "      <td>1.253642</td>\n",
       "      <td>0.014691</td>\n",
       "      <td>2.507789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>1.138971</td>\n",
       "      <td>-0.084689</td>\n",
       "      <td>1.616729</td>\n",
       "      <td>1.247900</td>\n",
       "      <td>0.014360</td>\n",
       "      <td>2.459913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>1.213512</td>\n",
       "      <td>0.021725</td>\n",
       "      <td>1.438897</td>\n",
       "      <td>1.241705</td>\n",
       "      <td>0.014931</td>\n",
       "      <td>2.293464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>1.095188</td>\n",
       "      <td>-0.012752</td>\n",
       "      <td>0.933299</td>\n",
       "      <td>1.236854</td>\n",
       "      <td>0.010924</td>\n",
       "      <td>2.353974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>1.123961</td>\n",
       "      <td>-0.095952</td>\n",
       "      <td>1.416283</td>\n",
       "      <td>1.232641</td>\n",
       "      <td>0.011140</td>\n",
       "      <td>2.302837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>1.115118</td>\n",
       "      <td>-0.012211</td>\n",
       "      <td>0.923878</td>\n",
       "      <td>1.229028</td>\n",
       "      <td>0.008674</td>\n",
       "      <td>2.298515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17</td>\n",
       "      <td>1.141572</td>\n",
       "      <td>-0.076293</td>\n",
       "      <td>1.558745</td>\n",
       "      <td>1.226231</td>\n",
       "      <td>0.007964</td>\n",
       "      <td>2.208467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18</td>\n",
       "      <td>1.083081</td>\n",
       "      <td>-0.010209</td>\n",
       "      <td>0.933583</td>\n",
       "      <td>1.223972</td>\n",
       "      <td>0.008731</td>\n",
       "      <td>2.103655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19</td>\n",
       "      <td>1.155902</td>\n",
       "      <td>-0.069891</td>\n",
       "      <td>1.515455</td>\n",
       "      <td>1.222110</td>\n",
       "      <td>0.007958</td>\n",
       "      <td>2.034258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20</td>\n",
       "      <td>1.114570</td>\n",
       "      <td>0.006574</td>\n",
       "      <td>0.916764</td>\n",
       "      <td>1.220826</td>\n",
       "      <td>0.007080</td>\n",
       "      <td>1.971585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21</td>\n",
       "      <td>1.084567</td>\n",
       "      <td>-0.008648</td>\n",
       "      <td>0.930361</td>\n",
       "      <td>1.219906</td>\n",
       "      <td>0.008020</td>\n",
       "      <td>1.892529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>22</td>\n",
       "      <td>1.101652</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.920995</td>\n",
       "      <td>1.219535</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>1.821086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>23</td>\n",
       "      <td>1.093592</td>\n",
       "      <td>-0.005340</td>\n",
       "      <td>0.924273</td>\n",
       "      <td>1.219598</td>\n",
       "      <td>0.007885</td>\n",
       "      <td>1.775616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     K  CH Score (K-Means)  Silhouette Score (K-Means)   \n",
       "0    2            1.203952                    0.046406  \\\n",
       "1    3            1.278319                   -0.004435   \n",
       "2    4            1.199890                    0.010224   \n",
       "3    5            1.153897                   -0.102977   \n",
       "4    6            1.166683                   -0.054472   \n",
       "5    7            1.150705                   -0.101835   \n",
       "6    8            1.156645                   -0.002306   \n",
       "7    9            1.185117                    0.013614   \n",
       "8   10            1.140845                   -0.095776   \n",
       "9   11            1.110942                   -0.115641   \n",
       "10  12            1.138971                   -0.084689   \n",
       "11  13            1.213512                    0.021725   \n",
       "12  14            1.095188                   -0.012752   \n",
       "13  15            1.123961                   -0.095952   \n",
       "14  16            1.115118                   -0.012211   \n",
       "15  17            1.141572                   -0.076293   \n",
       "16  18            1.083081                   -0.010209   \n",
       "17  19            1.155902                   -0.069891   \n",
       "18  20            1.114570                    0.006574   \n",
       "19  21            1.084567                   -0.008648   \n",
       "20  22            1.101652                    0.001428   \n",
       "21  23            1.093592                   -0.005340   \n",
       "\n",
       "    Davies-Bouldin Score (K-Means)  CH Score (Linkage)   \n",
       "0                         2.220843            1.337333  \\\n",
       "1                         4.913963            1.324281   \n",
       "2                         4.661496            1.320808   \n",
       "3                         4.439481            1.312459   \n",
       "4                         2.715054            1.301557   \n",
       "5                         3.686196            1.291308   \n",
       "6                         2.934716            1.278249   \n",
       "7                         2.404045            1.268345   \n",
       "8                         1.933558            1.260104   \n",
       "9                         1.742494            1.253642   \n",
       "10                        1.616729            1.247900   \n",
       "11                        1.438897            1.241705   \n",
       "12                        0.933299            1.236854   \n",
       "13                        1.416283            1.232641   \n",
       "14                        0.923878            1.229028   \n",
       "15                        1.558745            1.226231   \n",
       "16                        0.933583            1.223972   \n",
       "17                        1.515455            1.222110   \n",
       "18                        0.916764            1.220826   \n",
       "19                        0.930361            1.219906   \n",
       "20                        0.920995            1.219535   \n",
       "21                        0.924273            1.219598   \n",
       "\n",
       "    Silhouette Score (Linkage)  Davies-Bouldin Score (Linkage)  \n",
       "0                     0.036296                        3.559832  \n",
       "1                     0.031307                        2.464872  \n",
       "2                     0.031804                        1.720088  \n",
       "3                     0.012864                        3.438031  \n",
       "4                     0.012811                        3.306187  \n",
       "5                     0.014895                        3.055870  \n",
       "6                     0.014904                        2.996231  \n",
       "7                     0.014942                        2.753848  \n",
       "8                     0.014885                        2.561505  \n",
       "9                     0.014691                        2.507789  \n",
       "10                    0.014360                        2.459913  \n",
       "11                    0.014931                        2.293464  \n",
       "12                    0.010924                        2.353974  \n",
       "13                    0.011140                        2.302837  \n",
       "14                    0.008674                        2.298515  \n",
       "15                    0.007964                        2.208467  \n",
       "16                    0.008731                        2.103655  \n",
       "17                    0.007958                        2.034258  \n",
       "18                    0.007080                        1.971585  \n",
       "19                    0.008020                        1.892529  \n",
       "20                    0.008800                        1.821086  \n",
       "21                    0.007885                        1.775616  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cc947b",
   "metadata": {},
   "source": [
    "### Clean Up: Discussion\n",
    "Answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1ba98d",
   "metadata": {},
   "source": [
    "#### Which Smart City applicants did you remove? What issues did you see with the documents?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffebf5a5",
   "metadata": {},
   "source": [
    "Removed FL Tallahassee.pdf as it contains text in images and it is not possible for the pypdf to extract text from images.\n",
    "Removed NM Albuquerque.docx and GA Columbus.docx as they are in docx format not in pdf format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1620ed74",
   "metadata": {},
   "source": [
    "#### Explain what additional text processing methods you used and why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae42fc81",
   "metadata": {},
   "source": [
    "Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15817355",
   "metadata": {},
   "source": [
    "#### Did you identify any potientally problematic words?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ad6082",
   "metadata": {},
   "source": [
    "Nope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1507fbe",
   "metadata": {},
   "source": [
    "## Experimenting with Clustering Models\n",
    "\n",
    "Now, you'll start to explore models to find the optimal clustering model. In this section, you'll explore [K-means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), [Hierarchical](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html), and [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN) clustering algorithms.\n",
    "Create these algorithms with k_clusters for K-means and Hierarchical.\n",
    "For each cell in the table provide the [Silhouette score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score), [Calinski and Harabasz score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html#sklearn.metrics.calinski_harabasz_score), and [Davies-Bouldin score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html#sklearn.metrics.davies_bouldin_score).\n",
    "\n",
    "In each cell, create an array to store the values.\n",
    "For example, \n",
    "\n",
    "|Algorithm| k = 9 | k = 18| k = 36 | Optimal k| \n",
    "|--|--|--|--|--|\n",
    "|K-means| [S,CH,DB]| [S,CH,DB] | [S,CH,DB] | [S,CH,DB] |\n",
    "|Hierarchical |[S,CH,DB]| [S,CH,DB]| [S,CH,DB] | [S,CH,DB]|\n",
    "|DBSCAN | X | X | X | [S,CH,DB] |\n",
    "\n",
    "\n",
    "\n",
    "### Optimality \n",
    "You will need to find the optimal k for K-means and Hierarchical algorithms.\n",
    "Find the optimality for k in the range 2 to 50.\n",
    "Provide the code used to generate the optimal k and provide justification for your approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da2b033",
   "metadata": {},
   "source": [
    "|Algorithm| k = 9 | k = 18| k = 36 | Optimal k| \n",
    "|--|--|--|--|--|\n",
    "|K-means|--|--|--|--|\n",
    "|Hierarchical |--|--|--|--|\n",
    "|DBSCAN | X | X | X | -- |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d96d23a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c20fd69",
   "metadata": {},
   "source": [
    "#### How did you approach finding the optimal k?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c54da",
   "metadata": {},
   "source": [
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79ec635",
   "metadata": {},
   "source": [
    "#### What algorithm do you believe is the best? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3136b0a4",
   "metadata": {},
   "source": [
    "[Your Answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e45a2a3",
   "metadata": {},
   "source": [
    "### Add Cluster ID to output file\n",
    "In your data structure, add the cluster id for each smart city respectively. Show the to append the clusterid code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ad83e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "kmean = KMeans(n_clusters=13)\n",
    "kmean.fit(train_vector)\n",
    "cluster_labels = kmean.labels_\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "df['clusterid'] = cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a82c24e-d733-4ba9-a0aa-9b751d4163b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     4\n",
       "1     5\n",
       "2     9\n",
       "3     5\n",
       "4     5\n",
       "     ..\n",
       "63    1\n",
       "64    5\n",
       "65    5\n",
       "66    5\n",
       "67    5\n",
       "Name: clusterid, Length: 68, dtype: int32"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clusterid']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959e7275",
   "metadata": {},
   "source": [
    "### Save Model\n",
    "\n",
    "After finding the best model, it is desirable to have a way to persist the model for future use without having to retrain. Save the model using [model persistance](https://scikit-learn.org/stable/model_persistence.html). This model should be saved in the same directory as this notebook and should be loaded as the model for your `project3.py`.\n",
    "\n",
    "Save the model as `model.pkl`. You do not have to use pickle, but be sure to save the persistance using one of the methods listed in the link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c80938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(kmean, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe5a0c9",
   "metadata": {},
   "source": [
    "## Derving Themes and Concepts\n",
    "\n",
    "Perform Topic Modeling on the cleaned data. Provide the top five words for `TOPIC_NUM = Best_k` as defined in the section above. Feel free to reference [Chapter 6](https://github.com/dipanjanS/text-analytics-with-python/tree/master/New-Second-Edition/Ch06%20-%20Text%20Summarization%20and%20Topic%20Models) for more information on Topic Modeling and Summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a8027e5-e592-44f6-bb00-fb877aeb0623",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/akhila/.local/share/virtualenvs/cs5293sp23-project3-oUm7NRnr/lib/python3.10/site-packages (from scikit-learn) (1.10.1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(68, 67)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -U scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(min_df=20, max_df=0.6, ngram_range=(1,2),\n",
    "                     token_pattern=None, tokenizer=lambda doc: doc,\n",
    "                     preprocessor=lambda doc: doc)\n",
    "cv_features = cv.fit_transform(df['clean_text'])\n",
    "cv_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f553c92b-f138-44c3-840a-28a947bbb37c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size: 67\n"
     ]
    }
   ],
   "source": [
    "vocabulary = np.array(cv.get_feature_names_out())\n",
    "print('Total Vocabulary Size:', len(vocabulary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b684bc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: city transportation smart datum system\n",
      "Topic 1: city smart transportation system vehicle\n",
      "Topic 2: city smart datum vehicle use\n",
      "Topic 3: city smart transportation system datum\n",
      "Topic 4: city smart transportation vehicle system\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Convert text data into matrix of token counts\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "# Perform topic modeling using Latent Dirichlet Allocation (LDA)\n",
    "lda_model = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "lda_model.fit(X)\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the top 5 words for each topic\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    top_words_idx = topic.argsort()[:-6:-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(f\"Topic {topic_idx}: {' '.join(top_words)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915516dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2a3896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1573fe65",
   "metadata": {},
   "source": [
    "### Extract themes\n",
    "Write a theme for each topic (atleast a sentence each)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9737d8df",
   "metadata": {},
   "source": [
    "[Your Answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cf24c6",
   "metadata": {},
   "source": [
    "[Your Answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e191f4",
   "metadata": {},
   "source": [
    "[Your Answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ab7df0",
   "metadata": {},
   "source": [
    "### Add Topid ID to output file\n",
    "Add the top two topics for each smart city to the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e937841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Create a dictionary to store top topics for each city\n",
    "city_topics = {}\n",
    "\n",
    "\n",
    "city_topics[city] = []\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    top_words_idx = topic.argsort()[:-6:-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    city_topics[city].append({'topic': topic_idx, 'words': top_words})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8e568652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lubbock TX': [{'topic': 0,\n",
       "   'words': ['city', 'new', 'smart', 'datum', 'use']},\n",
       "  {'topic': 1, 'words': ['city', 'new', 'smart', 'datum', 'use']}]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671ec7bd-cf0a-423a-aa2b-886147cef77f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39f9c240",
   "metadata": {},
   "source": [
    "## Gathering Applicant Summaries and Keywords\n",
    "\n",
    "For each smart city applicant, gather a summary and keywords that are important to that document. You can use gensim to do this. Here are examples of functions that you could use.\n",
    "\n",
    "```python\n",
    "\n",
    "from gensim.summarization import summarize\n",
    "\n",
    "def summary(text, ratio=0.2, word_count=250, split=False):\n",
    "    return summarize(text, ratio= ratio, word_count=word_count, split=split)\n",
    "    \n",
    "from gensim.summarization import keywords\n",
    "\n",
    "def keys(text, ratio=0.01):\n",
    "    return keywords(text, ratio=ratio)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86cbe26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b545ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d27ce37",
   "metadata": {},
   "source": [
    "### Add Summaries and Keywords\n",
    "Add summary and keywords to output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09357ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f9cb2c4",
   "metadata": {},
   "source": [
    "## Write output data\n",
    "\n",
    "The output data should be written as a TSV file.\n",
    "You can use `to_csv` method from Pandas for this if you are using a DataFrame.\n",
    "\n",
    "`Syntax: df.to_csv('file.tsv', sep = '')` \\\n",
    "`df.to_csv('smartcity_eda.tsv', sep='\\t')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "58827464",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('smartcity_eda.tsv', sep='\\t', escapechar='\\\\')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a18b8ff",
   "metadata": {},
   "source": [
    "# Moving Forward\n",
    "Now that you have explored the dataset, take the important features and functions to create your `project3.py`.\n",
    "Please refer to the project spec for more guidance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6675ba",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
